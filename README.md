# Job Role Prediction System

A comprehensive machine learning system for predicting job roles based on candidate skills, qualifications, and experience levels using advanced feature engineering with word embeddings, semantic transformers, and XGBoost.

## ğŸ¯ Project Overview

This system predicts the most suitable job role for a candidate by analyzing:
- **Skills**: Semantic representation using Word2Vec embeddings
- **Qualifications**: Hybrid approach combining educational hierarchy and semantic field embeddings
- **Experience Level**: Ordinal encoding for career progression

## ğŸ“Š Dataset

**Location**: `datasets/candidate_job_role_dataset.csv`

**Statistics**:
- 1,000 candidate profiles
- 25+ unique job roles
- 100+ unique skills
- Features: candidate_id, skills, qualification, experience_level, job_role

## ğŸ—ï¸ Project Structure

```
ECS171ResumeAnalysis/
â”‚
â”œâ”€â”€ artifacts/                          # Trained models and encoders
â”‚   â”œâ”€â”€ model.joblib
â”‚   â”œâ”€â”€ standard_scaler.joblib
â”‚   â”œâ”€â”€ qualification_ordinal_encoder.joblib
â”‚   â”œâ”€â”€ experience_ordinal_encoder.joblib
â”‚   â””â”€â”€ label_encoder.joblib
â”‚
â”œâ”€â”€ datasets/                           # Data files
â”‚   â””â”€â”€ candidate_job_role_dataset.csv
â”‚
â”œâ”€â”€ notebooks/                          # Jupyter notebooks
â”‚   â””â”€â”€ 1-data-exploration.ipynb
â”‚
â”œâ”€â”€ src/                                # Source code
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py          # Data loading and splitting
â”‚   â”‚   â”œâ”€â”€ data_transformation.py     # Feature engineering
â”‚   â”‚   â””â”€â”€ model_trainer.py           # Model training and evaluation
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py          # Complete training workflow
â”‚   â”‚   â””â”€â”€ predict_pipeline.py        # Inference pipeline
â”‚   â”‚
â”‚   â””â”€â”€ utils.py                       # Utility functions
â”‚
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                          # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- pip

### Installation

1. **Clone the repository** (if applicable)
   ```bash
   cd ECS171ResumeAnalysis
   ```

2. **Create a virtual environment**
   ```bash
   python3 -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

   **Note**: First-time installation will download large pre-trained models:
   - Sentence Transformer (all-MiniLM-L6-v2): ~90 MB
   - Word2Vec (word2vec-google-news-300): ~1.5 GB

## ğŸ“š Usage

### 1. Data Exploration (Optional)

Explore the dataset interactively:

```bash
jupyter notebook notebooks/1-data-exploration.ipynb
```

This notebook includes:
- Data profiling and quality checks
- Distribution visualizations
- Skills analysis and word clouds
- Class imbalance analysis
- Feature engineering prototypes
- Baseline model testing

### 2. Training the Model

Run the complete training pipeline:

```bash
cd src/pipeline
python3 train_pipeline.py
```

**What happens during training:**
1. **Data Ingestion**: Loads data and splits into train/val/test (70%/15%/15%)
2. **Feature Engineering**:
   - Qualification: Ordinal hierarchy + semantic embeddings (384-dim)
   - Experience: Ordinal encoding (Entry=1, Mid=2, Senior=3)
   - Skills: Averaged Word2Vec embeddings (300-dim)
3. **Model Training**: XGBoost with RandomizedSearchCV (50 iterations, 3-fold CV)
4. **Evaluation**: Classification report on test set
5. **Artifact Saving**: All models and encoders saved to `artifacts/`

**Expected output:**
```
================================================================================
TRAINING PIPELINE COMPLETED SUCCESSFULLY!
================================================================================

Final Model Performance:
  â€¢ Test Accuracy: 0.85+ (varies)
  â€¢ Test F1 Score (Weighted): 0.85+
  â€¢ Test F1 Score (Macro): 0.84+
  â€¢ Cross-validation F1 Score: 0.83+
```

### 3. Making Predictions

#### Option A: Demo Script

Run the demo with example candidates:

```bash
cd src/pipeline
python3 predict_pipeline.py
```

#### Option B: Custom Predictions

Use the prediction pipeline in your code:

```python
from src.pipeline.predict_pipeline import PredictionPipeline

# Initialize pipeline (loads all artifacts)
pipeline = PredictionPipeline()

# Make a prediction
result = pipeline.predict_role(
    skills="Python, TensorFlow, Machine Learning, Deep Learning, NLP",
    qualification="Master's in Data Science",
    experience_level="Senior"
)

print(f"Predicted Role: {result['predicted_role']}")
print(f"Suitability Score: {result['suitability_score']:.2%}")
print(f"Top 3 Predictions: {result['top_3_predictions']}")
```

**Example output:**
```
================================================================================
Predicting for: Sample Candidate
--------------------------------------------------------------------------------
Skills: Python, TensorFlow, Machine Learning, Deep Learning, NLP
Qualification: Master's in Data Science
Experience: Senior
--------------------------------------------------------------------------------

                        PREDICTION RESULTS                        
================================================================================
Predicted Job Role: Data Scientist
Suitability Score: 92.45%

Top 3 Predictions:
  1. Data Scientist: 92.45%
  2. AIML: 4.23%
  3. Data Analyst: 1.87%
================================================================================
```

## ğŸ”¬ Feature Engineering Details

### Hybrid Qualification Encoding

**1. Educational Hierarchy (Ordinal)**
- Maps degree levels: High School(1) â†’ Bachelor's(2) â†’ Master's(3) â†’ PhD(4)
- Captures career progression requirements

**2. Semantic Field Embeddings**
- Uses Sentence Transformers (all-MiniLM-L6-v2)
- Generates 384-dimensional embeddings
- Captures similarity between fields (e.g., "Computer Science" â‰ˆ "Software Engineering")

### Skill Vector Generation

- Uses Word2Vec (word2vec-google-news-300)
- Generates 300-dimensional averaged embeddings
- Handles multi-word skills (e.g., "Machine Learning")
- Captures semantic relationships (e.g., "Python" â‰ˆ "JavaScript")

### Standard Scaling

- Applied to all combined features
- Fitted on training data only
- Ensures consistent feature scales

## ğŸ¯ Model Architecture

**Algorithm**: XGBoost Classifier
- Multi-class classification (25+ job roles)
- Hyperparameter tuning with RandomizedSearchCV
- 50 random combinations tested
- 3-fold cross-validation
- Optimization metric: F1 Score (weighted)

**Hyperparameter Search Space**:
```python
{
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.2]
}
```

## ğŸ“ˆ Performance Metrics

The system is evaluated using:
- **Accuracy**: Overall correctness
- **F1 Score (Weighted)**: Accounts for class imbalance
- **F1 Score (Macro)**: Equal weight to all classes
- **Classification Report**: Per-class precision, recall, and F1

Expected performance: **85-90% accuracy** on test set

## ğŸ”§ Saved Artifacts

All artifacts are saved in `artifacts/` directory:

| Artifact | Description | Size |
|----------|-------------|------|
| `model.joblib` | Trained XGBoost model | ~5-10 MB |
| `standard_scaler.joblib` | Feature scaler | <1 MB |
| `qualification_ordinal_encoder.joblib` | Degree level encoder | <1 MB |
| `experience_ordinal_encoder.joblib` | Experience encoder | <1 MB |
| `label_encoder.joblib` | Job role encoder | <1 MB |

## ğŸ“Š Key Insights

From data exploration:
- **Balanced dataset**: ~40 candidates per job role
- **Common skills**: Python, SQL, JavaScript, AWS, Docker
- **Education**: Majority have Bachelor's or Master's degrees
- **Experience**: Distributed across Entry, Mid, and Senior levels

## ğŸ”® Future Enhancements

- [ ] Create web interface for predictions
- [ ] Add model versioning and tracking

## ğŸ“ License

This project is part of the ECS171 Machine Learning course at UC Davis.

## ğŸ‘¥ Contributors

Team 5 - ECS171 Fall Quarter 2025

## ğŸ™ Acknowledgments

- Pre-trained models: Sentence Transformers, Google Word2Vec
- Libraries: scikit-learn, XGBoost, gensim, pandas
- Course: ECS171 - Machine Learning, UC Davis

